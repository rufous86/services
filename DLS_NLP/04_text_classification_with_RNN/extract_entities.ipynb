{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "AwXmxZbtQA8T"
   },
   "outputs": [],
   "source": [
    "!pip install natasha -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "zbwMe6bURSSr"
   },
   "outputs": [],
   "source": [
    "text_list = ['Я вчера купил кроссовки Nike.',\n",
    "             'А потом еще вафли Valio', \n",
    "             'Еще нужно купить пачку пельменей',\n",
    "             'Что может быть вкуснее Фрутоняня?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h-jnlP5POm0x",
    "outputId": "4f282e3d-562a-44d0-8939-eba54df24575"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Я вчера купил кроссовки Nike.', 'А потом еще вафли Valio', 'Еще нужно купить пачку пельменей', 'Что может быть вкуснее Фрутоняня?']\n",
      "{'Я вчера купил кроссовки Nike.': 'Nike', 'А потом еще вафли Valio': 'Valio'}\n",
      "{'Я вчера купил кроссовки Nike.': 'кроссовок', 'А потом еще вафли Valio': 'вафля', 'Еще нужно купить пачку пельменей': 'пельмень'}\n"
     ]
    }
   ],
   "source": [
    "import natasha as ntsh\n",
    "from natasha import (\n",
    "    Segmenter,\n",
    "    NewsEmbedding,\n",
    "    NewsNERTagger,\n",
    "    MorphVocab,\n",
    "    NewsMorphTagger,\n",
    "    Doc\n",
    ")\n",
    "\n",
    "segmenter = Segmenter()\n",
    "emb = NewsEmbedding()\n",
    "ner_tagger = NewsNERTagger(emb)\n",
    "morph_tagger = NewsMorphTagger(emb)\n",
    "morph_vocab = MorphVocab()\n",
    "\n",
    "def extract_brands(text_list: list):\n",
    "    \"\"\"\n",
    "    Функция для извлечения названий брендов из текста\n",
    "    \"\"\"\n",
    "    res = {}\n",
    "    for sent in text_list:\n",
    "        doc = Doc(sent)\n",
    "        doc.segment(segmenter)\n",
    "        doc.tag_ner(ner_tagger)\n",
    "        for span in doc.spans:\n",
    "            # print(span, span.type)\n",
    "            if span.type == 'ORG':\n",
    "                res[sent] = span.text\n",
    "    return res\n",
    "\n",
    "def extract_nouns(text_list: list):\n",
    "    \"\"\"\n",
    "    Функция для извлечения только существительных из текста\n",
    "    \"\"\"\n",
    "    res = {}\n",
    "    for sent in text_list:\n",
    "        doc = Doc(sent)\n",
    "        doc.segment(segmenter)\n",
    "        doc.tag_morph(morph_tagger)\n",
    "        for token in doc.tokens:\n",
    "            token.lemmatize(morph_vocab)\n",
    "            if token.pos == 'NOUN':\n",
    "                res[sent] = token.lemma\n",
    "    return res\n",
    "\n",
    "print(text_list)\n",
    "print(extract_brands(text_list))\n",
    "print(extract_nouns(text_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "93SqxfGmfYWo"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
