{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install natasha -q"
      ],
      "metadata": {
        "id": "AwXmxZbtQA8T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_list = ['Я вчера купил кроссовки Nike.',\n",
        "             'А потом еще вафли Valio', \n",
        "             'Еще нужно купить пачку пельменей',\n",
        "             'Что может быть вкуснее Фрутоняня?']"
      ],
      "metadata": {
        "id": "zbwMe6bURSSr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import natasha as ntsh\n",
        "from natasha import (\n",
        "    Segmenter,\n",
        "    NewsEmbedding,\n",
        "    NewsNERTagger,\n",
        "    MorphVocab,\n",
        "    NewsMorphTagger,\n",
        "    Doc\n",
        ")\n",
        "\n",
        "segmenter = Segmenter()\n",
        "emb = NewsEmbedding()\n",
        "ner_tagger = NewsNERTagger(emb)\n",
        "morph_tagger = NewsMorphTagger(emb)\n",
        "morph_vocab = MorphVocab()\n",
        "\n",
        "def extract_brands(text_list: list):\n",
        "    \"\"\"\n",
        "    Функция для извлечения названий брендов из текста\n",
        "    \"\"\"\n",
        "    res = {}\n",
        "    for sent in text_list:\n",
        "        doc = Doc(sent)\n",
        "        doc.segment(segmenter)\n",
        "        doc.tag_ner(ner_tagger)\n",
        "        for span in doc.spans:\n",
        "            # print(span, span.type)\n",
        "            if span.type == 'ORG':\n",
        "                res[sent] = span.text\n",
        "    return res\n",
        "\n",
        "def extract_nouns(text_list: list):\n",
        "    \"\"\"\n",
        "    Функция для извлечения только существительных из текста\n",
        "    \"\"\"\n",
        "    res = {}\n",
        "    for sent in text_list:\n",
        "        doc = Doc(sent)\n",
        "        doc.segment(segmenter)\n",
        "        doc.tag_morph(morph_tagger)\n",
        "        for token in doc.tokens:\n",
        "            token.lemmatize(morph_vocab)\n",
        "            if token.pos == 'NOUN':\n",
        "                res[sent] = token.lemma\n",
        "    return res\n",
        "\n",
        "print(text_list)\n",
        "print(extract_brands(text_list))\n",
        "print(extract_nouns(text_list))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-jnlP5POm0x",
        "outputId": "4f282e3d-562a-44d0-8939-eba54df24575"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Я вчера купил кроссовки Nike.', 'А потом еще вафли Valio', 'Еще нужно купить пачку пельменей', 'Что может быть вкуснее Фрутоняня?']\n",
            "{'Я вчера купил кроссовки Nike.': 'Nike', 'А потом еще вафли Valio': 'Valio'}\n",
            "{'Я вчера купил кроссовки Nike.': 'кроссовок', 'А потом еще вафли Valio': 'вафля', 'Еще нужно купить пачку пельменей': 'пельмень'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "93SqxfGmfYWo"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}